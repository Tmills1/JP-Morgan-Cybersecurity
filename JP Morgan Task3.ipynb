{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {
    "id": "1df29eda"
   },
   "source": [
    "Step 0. Unzip enron1.zip into the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de147c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('enron1.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {
    "id": "bf32cfce"
   },
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c5d195",
   "metadata": {
    "id": "20c5d195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 2649.2004-10-27.GP.spam.txt\n",
      "skipped 0754.2004-04-01.GP.spam.txt\n",
      "skipped 2042.2004-08-30.GP.spam.txt\n",
      "skipped 3304.2004-12-26.GP.spam.txt\n",
      "skipped 4142.2005-03-31.GP.spam.txt\n",
      "skipped 3364.2005-01-01.GP.spam.txt\n",
      "skipped 4201.2005-04-05.GP.spam.txt\n",
      "skipped 2140.2004-09-13.GP.spam.txt\n",
      "skipped 2248.2004-09-23.GP.spam.txt\n",
      "skipped 4350.2005-04-23.GP.spam.txt\n",
      "skipped 4566.2005-05-24.GP.spam.txt\n",
      "skipped 2526.2004-10-17.GP.spam.txt\n",
      "skipped 1414.2004-06-24.GP.spam.txt\n",
      "skipped 2698.2004-10-31.GP.spam.txt\n",
      "skipped 5105.2005-08-31.GP.spam.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/fzqpns5n1z31_c8mnnyn02yw0000gn/T/ipykernel_80766/217166261.py:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame.from_records(spam))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = './enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.append({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham = read_ham()\n",
    "spam = read_spam()\n",
    "\n",
    "df = pd.DataFrame.from_records(ham)\n",
    "df = df.append(pd.DataFrame.from_records(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {
    "id": "1a1c23fd"
   },
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c447c901",
   "metadata": {
    "id": "c447c901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  this is a test string       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Replace all non-alphabetic characters with a space using regex\n",
    "    normalized_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Convert the result to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Test the preprocessor function\n",
    "input_text = \"Hello, this is a Test-String 123!@#\"\n",
    "processed_text = preprocessor(input_text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {
    "id": "ba32521d"
   },
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1442d377",
   "metadata": {
    "id": "1442d377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8478260869565217\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    0    1]\n",
      " [   0 1455   10]\n",
      " [   0  304  300]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      enron1       0.00      0.00      0.00         1\n",
      "         ham       0.83      0.99      0.90      1465\n",
      "        spam       0.96      0.50      0.66       604\n",
      "\n",
      "    accuracy                           0.85      2070\n",
      "   macro avg       0.60      0.50      0.52      2070\n",
      "weighted avg       0.87      0.85      0.83      2070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/tyler/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tyler/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tyler/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 0: Create a list to store the text data and labels\n",
    "text_data = []\n",
    "labels = []\n",
    "\n",
    "# Step 1: Read the text data and labels from the dataset\n",
    "data_dir = \"enron_data\"  # Assuming the subdirectories 'ham' and 'spam' are inside the 'enron_data' directory\n",
    "\n",
    "# Traversing through subdirectories using os.walk()\n",
    "for root, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            label = os.path.basename(root)  # The subdirectory name will be used as the label\n",
    "            labels.append(label)\n",
    "            with open(os.path.join(root, filename), 'r', encoding='latin-1') as file:\n",
    "                text_data.append(file.read())\n",
    "\n",
    "# Additional check to ensure that text data is not empty\n",
    "if len(text_data) == 0:\n",
    "    raise ValueError(\"No text data found in the subdirectories. Please check the file names and path.\")\n",
    "\n",
    "# Step 2: Instantiate a CountVectorizer with the preprocessor function\n",
    "def preprocessor(text):\n",
    "    # Replace all non-alphabetic characters with a space using regex\n",
    "    normalized_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Convert the result to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "# Step 3: Use train_test_split to split the dataset into a train dataset and a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Use the vectorizer to transform the existing dataset into a form in which the model can learn from\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 5: Use the LogisticRegression model to fit to the train dataset\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Step 6: Validate that the model has learned something by generating predictions\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Step 7: Evaluate the model's performance using the provided functions\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {
    "id": "9674d032"
   },
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b7d78c9",
   "metadata": {
    "id": "6b7d78c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Positive Features (Spam):\n",
      "bbcff: -9.680341120615691e-06\n",
      "clickhere: -9.680341120615691e-06\n",
      "desperately: -9.680341120615691e-06\n",
      "hotties: -9.680341120615691e-06\n",
      "htmlsee: -9.680341120615691e-06\n",
      "pleasuring: -9.680341120615691e-06\n",
      "strudel: -9.680341120615691e-06\n",
      "addressbr: -9.787404087758373e-06\n",
      "creaming: -9.787404087758373e-06\n",
      "eac: -9.787404087758373e-06\n",
      "\n",
      "Top 10 Negative Features (Ham):\n",
      "subject: -0.8167546498932253\n",
      "the: -0.2775231883810057\n",
      "be: -0.2772567914185057\n",
      "com: -0.26813654974471407\n",
      "to: -0.23153286920598076\n",
      "os: -0.213705068225571\n",
      "ea: -0.2123600316173644\n",
      "apple: -0.21180523204023968\n",
      "bd: -0.21156904238526372\n",
      "mac: -0.21142280088598905\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Get the features (words) created by the vectorizer\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 5: Get the coefficients (importance) from the model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Step 6: Find the top 10 positive features with the largest magnitude (corresponding to spam)\n",
    "top_positive_features = sorted(zip(features, coefficients), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Step 7: Find the top 10 negative features with the largest magnitude (corresponding to ham)\n",
    "top_negative_features = sorted(zip(features, coefficients), key=lambda x: x[1])[:10]\n",
    "\n",
    "# Print the top positive and negative features\n",
    "print(\"Top 10 Positive Features (Spam):\")\n",
    "for feature, coefficient in top_positive_features:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print(\"\\nTop 10 Negative Features (Ham):\")\n",
    "for feature, coefficient in top_negative_features:\n",
    "    print(f\"{feature}: {coefficient}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {
    "id": "d267e7ad"
   },
   "source": [
    "Submission\n",
    "1. Upload the jupyter notebook to Forage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LI4u_ZUGToDQ",
   "metadata": {
    "id": "LI4u_ZUGToDQ"
   },
   "source": [
    "All Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
